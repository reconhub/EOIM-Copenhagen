---
title: Outbreak of gastroenteritis after a high school dinner in Copenhagen, Denmark,November
  2006
author: "Alexander Spina and Patrick Keating"
output:
  pdf_document: default
  word_document: default
geometry: margin=1.5cm
---
The following code has been adapted to *R* for learning purposes. The initial contributors and copyright license are listed below. All copyrights and licenses of the original document apply here as well. 

**Contributors to *R* code:**  
Daniel Gardiner(PHE) and Lukas Richter (AGES)

# Copyright and license
This case study was designed under an ECDC service contract for the development of training material (2010). The data were slightly modified for training purposes.

**Source :**
This case study is based on an investigation conducted by Jurgita Pakalniskiene, Gerhard Falkenhorst (Statens Serum Institut, Copenhagen) and colleagues

**Authors:**
Jurgita Pakalniskiene, Gerhard Falkenhorst, Esther Kissling, Gilles Desvé.

**Reviewers:**
Marta Valenciano, Alain Moren.

**Adaptions for previous modules:**
Irina Czogiel, Kristin Tolksdorf, Michaela Diercke, Sybille Somogyi, Christian Winter, Sandra Dudareva-Vizule with the help of Katharina Alpers, Alicia Barrasa, Androulla Efstratiou, Steen Ethelberg, Annette Heißenhuber, Aftab Jasir, Ioannis Karagiannis and Pawel Stefanoff


**You are free:**

+ to Share - to copy, distribute and transmit the work
+ to Remix - to adapt the work
Under the following conditions:
+ Attribution - You must attribute the work in the manner specified by the author or licensor (but not in any way that suggests that they endorse you or your use of the work). The best way to do this is to keep as it is the list of contributors: sources, authors and reviewers.
+ Share Alike - If you alter, transform, or build upon this work, you may distribute the resulting work only under the same or similar license to this one. Your changes must be documented. Under that condition, you are allowed to add your name to the list of contributors.
+ You cannot sell this work alone but you can use it as part of a teaching.
With the understanding that:
+ Waiver - Any of the above conditions can be waived if you get permission from the copyright holder.
+ Public Domain - Where the work or any of its elements is in the public domain under applicable law, that status is in no way affected by the license.
+ Other Rights - In no way are any of the following rights affected by the license:
+ Your fair dealing or fair use rights, or other applicable copyright exceptions and limitations;
+ The author's moral rights;
+ Rights other persons may have either in the work itself or in how the work is used, such as publicity or privacy rights.
+ Notice - For any reuse or distribution, you must make clear to others the license terms of this work by keeping together this work and the current license.
This licence is based on http://creativecommons.org/licenses/by-sa/3.0/

\pagebreak 


# The Alert 

On November 14th 2006 the director of a high school in Greater Copenhagen, Denmark, contacted the regional public health authorities to inform them about an outbreak of diarrhoea and vomiting among participants from a school dinner party held on the 11th of November 2006. Almost all students and teachers of the school (750 people) attended the party.  
The first people fell ill the same night and by the November 14th the school had received reports of diarrhoeal illness from around 200 - 300 students and teachers, many of whom also reported vomiting.  

The epidemiologists in the outbreak team decided to perform a retrospective cohort study in order to identify the food item that was the vehicle of the outbreak. The cohort was defined as students and teachers who had attended the party at the high school on 11th of November 2006.  
A questionnaire was designed to conduct a survey on food consumption and on presentation of the illness. Information about the survey and a link to the questionnaire was circulated to students and teachers via the school's intranet with the request that everyone who attended the school party on 11th of November 2006 should fill in the questionnaire.  
Practically all students and teachers check the intranet on a daily basis, because it is the school's main communication channel for information about courses, homework assignments, cancellation of lessons etc. The information about the investigation was therefore also displayed on the screen in the main hall of the school. The school's intranet was also accessible for ill students or teachers from home so that everyone in the cohort could potentially participate and the response rate could be maximised.  
Symptomatic party attendees were asked to submit stool samples via their general practitioners to the local clinical microbiology laboratory where they were cultured for standard enteric bacteria. The working hypothesis at the point was that the outbreak had a viral or toxic aetiology. Norovirus is generally acknowledged as the most frequent cause of foodborne outbreaks in industrialised countries, so this would be a prime suspect.  
In the study, the following case definition was used: a case is a person from the cohort, who presented with diarrhoea or vomiting within 48 hours of the meal. So anyone who presented with diarrhoea or vomiting from 6pm on November 11th to 5:59pm on November 13th was included as a case. Anyone with symptoms outside this time window is defined as a control as this person probably didn't become sick at the party.


\pagebreak

# An introduction to the R companion 


### Your R Workflow

We will be using RStudio projects to separate our sessions into distinct workflows.
For each session, you will open R by double-clicking on the `.Rproj` file, or opening RStudio and selecting the session from the upper-right-hand dropdown menu.

> Just as in STATA you can set a folder to be your working directory (using the setwd command), but we will not be using this because it makes transferring between computers difficult (see <https://www.tidyverse.org/articles/2017/12/workflow-vs-script/> for details). Instead, we will be using the RStudio project files and the 'here' package to keep track of files.

For this case study I have put an "MSF Training" folder on my desktop and organised it in to sub-folders for each session.

<!-- ZNK: This needs to change to be updated to the current course -->

> nb.you can of course organise your files however you want and set your working directory accordingly - the simplest case is to put all datasets in a folder called "Outbreak module 2015", and not separate in to sessions

> side note: see appendix if you would like to read in STATA datafiles directly*


### Installing packages and functions

R packages are bundles of functions which extend the capability of R. Thousands of add-on packages are available in the main online repository (known as CRAN) and many more packages in development can be found on GitHub. They may be installed and updated over the Internet.

We will mainly use packages which come ready installed with R (base code), but where it makes things easier we will use add-on packages. In addition, we have included a few extra functions to simplify the code required. All the R packages you need for the exercises can be installed over the Internet.


```{r, eval=FALSE, results='hide', message=FALSE, warning=FALSE}
# Installing required packages for the week
required_packages <- c("readxl", "haven", "ggplot2", "Hmisc", "epiDisplay", "epiR") 
install.packages(required_packages)
```


Run the following code at the beginning of the day to make sure that you have made available all the packages and functions that you need. Be sure to include it in any scripts too.

```{r, echo = TRUE, results='hide', message=FALSE, warning=FALSE}
# Loading required packages for the week
library("readxl")
library("haven")
library("ggplot2")
library("Hmisc")
library("epiDisplay")
library("epiR")
library("knitr")
library("here")
```

```{r}
# Functions required
# Adds a function to create epicurves
source(here("scripts", "epicurve.v.1.8.R")) 

# Adds a function to create output similar to cctable or cstable in Stata
source(here("scripts", "single.variable.analysis.v0.2.R")) 
```

R and Stata have minor differences in default settings and methods. In this document we will follow the Stata analysis as closely as possible, but small and usually unimportant differences may be noted between the statistical findings in R and those in Stata. At some points additional steps (which would usually be optional in R) will be taken to produce output which is comparable to that of Stata.

The epicurve function allows creation of easily formatted epicurves. To find out more about the function, first load it as above and then click on function in the **Global Environment** tab on the right of the R Studio window. The **single variable analysis** function allows calculation of attack rates of multiple variables at one time and provides similar output to the cctable and cstable commands in Stata.

You will work with Stata.dta data sets which can be loaded into R with the "haven" or "readstata13" packages. The appropriate functions to use will be indicated.

R can hold one or many data sets in memory simultaneously, so there is usually no need to save intermediate files or close and re-open datasets.


\pagebreak 


# Data management and *R scripts*
## preparing the script


### Reading in datasets
Open the data set **copenhagen3** using the *read.csv* command.  
This dataset has been exported from the STATA *.dta* file and saved as a *.csv* file.  
It is also possible to import datasets from other formats, such as excel; see appendix for example. 
Datasets in *R* are stored and can be referred to using the name it is saved as (in our case "cph").


```{r}
# read in your data from a csv file 
# Select separator as comma (sep=",")
# do not import 'string' variables as 'Factors' (stringsAsFactors=FALSE) 
# Factors are a special datatype, covered later - character variables are simpler
# data frame read in and saved in R as "cph"


cph <- read.csv(here("Stata_data", "copenhagen_raw.csv"), sep=",", stringsAsFactors = FALSE) 
```

### Browsing your dataset 
*R studio* has the nice feature that everything is in one browser window, so you can browse your dataset and your code without having to switch between browser windows. 

```{r, eval = FALSE}
# to browse your data, use the View command
View(cph)
```

Alternatively, you can also view your dataset by clicking on *cph* in the top right "global environment" panel of your *R studio* browser.  
Your global environment is where you can see all the datasets, functions and other things you have running in the current session. (see figure 1 below)

![Browsing your dataset in R studio](Screenshot1.png) <!-- TODO: find this screenshot -->


### Saving your code in R Scripts
R scripts are the equivalent of .do files in STATA. You can save your code in these R scripts. (see figure 2 below) 
You can write comments in your code using "# "

![Creating a new R-script in R studio](Screenshot2.png)  <!-- TODO: find this screenshot -->

## Log-files 

These do not exist in R, however there is the History tab, in the global environment panel in the top right of your browser.
If you click on the code in the History tab, it will re-run.


### Describing your dataset 
You can view the structure of your data set using the following commands. Each of these commands can be run for individual variables also. You can refer to an individual variable of a data set by using the `$`, for example, if you wanted to obtain a summary of the age variable, then you would write `summary(cph$age)`.  

```{r, eval = FALSE}
# str provides an overview of the number of observations and variable types
str(cph)

# summary provides information of variable class as well as extra details for numeric variables
summary(cph)

# describe (from Hmisc package) provides no. of observations, missing values, unique levels of each variable
Hmisc::describe(cph) 
```

> **n.b.** we are using the convention package::function() to make it clear when we are using a function that comes from an external package.

# Data cleaning and recoding in *R*  

## Check the dataset "Copenhagen.csv"


Using the "table" and "summary" commands, you can get the equivalent of "tabulate" and "summarize" in STATA  
In the example below we look at age in the cph dataset.  

You can examine a variable within a dataset using the `$` sign and then the variable name (e.g. `cph$age`). 
Alternatively, you can also refer to a dataset using square brackets, the part before a comma refers to the rows and after refers to columns (named or numerically). Or visually: `cph[row , column]`.
For example: `cph[ , "age"]` gives you the variable age as a vector, or `cph[2:4 , ]` gives you rows two to four. 

You can subset a dataset using `[...]` in combination with double-equals (`==`), does not equal (`!=`), and less or greater than (`<`, `>`).
    
In R, when defining the filter this can be both numerical or text (i.e. the gender example). 
In order to combine multiple filtering commands in to one selection you can use the "`|`" (bar not capital i) or "`&`" symbols. 
The **`|`** stands for **or** whereas the **`&`** stands for **and**. 

To select cases which are empty, use `is.na()`, for those which are not, `!is.na(...)`. 
The exclamation mark ("`!`") implies "not" in this situation. 

New variables are easily created in R simply by using the $ sign after the dataset name and writing a name not already in the dataset, then defining what should go in that variable.




```{r}
# table will give a very basic frequency table (counts), 
# in this example the first line of the output is the age and the second is the frequency.
table(cph$age)

# summary gives you more detailed statistics as in stata
summary(cph$age)
```


\pagebreak 

```{r}
# You can look at age among teachers using the group variable
table(cph$age[cph$group == 0])

### Things you may have noticed: 

# an outlier in incubation
summary(cph$incubation) 

# people did not have dinner but ate tuna, bread or veal
  # you can label the table by adding labels
table(meal = cph$meal, tuna = cph$tuna)
table(meal = cph$meal, bread = cph$bread)
table(meal = cph$meal, veal = cph$veal)

# people with day of onset but no symptoms
  # is.na() returns True/False if value is missing
  # So this selects the participants that are either 0 or missing for all three symptoms
no_symptoms <- function(x){
  x != 1 | is.na(x)
}

table(cph$start[no_symptoms(cph$diarrhoea) & 
                no_symptoms(cph$vomiting)  &
                no_symptoms(cph$bloody)    ])
# We can also gather the data first
subset_of_no_symptoms <- no_symptoms(cph$diarrhoea)
subset_of_no_symptoms <- subset_of_no_symptoms & no_symptoms(cph$vomiting)
subset_of_no_symptoms <- subset_of_no_symptoms & no_symptoms(cph$bloody)
table(cph$start[subset_of_no_symptoms])

# We can also gather everything into one command, but this is a bit uglier
# and more prone to error
table(cph$start[(cph$diarrhoea != 1 | is.na(cph$diarrhoea)) & 
                  (cph$vomiting != 1 | is.na(cph$vomiting) & 
                     (cph$bloody != 1 | is.na(cph$bloody))) ])
```



\pagebreak

## Data cleaning 

### Recoding missing values, age and sex 



```{r}

# correct mistakes in age
cph$age[cph$age == 8] <- 18
cph$age[cph$age == 180] <- 18

# calculate your own age variable based on birthday
  # Choose the first day of symptom onset as date to calculate from
cph$age2 <- (as.Date("2006-11-11") - as.Date(cph$birthday)) / 365.25
# change this to a number and round it
cph$age2 <- round(as.numeric(cph$age2), digits = 0)


# correct mistake in incubation time 
cph$incubation[cph$incubation == 210] <- 21

# Correct mistake regarding onset start
  # Those who have no symptoms should also not have an onset date
  # So this is selecting those that are 0 or empty for each of the three symptoms
cph$start[(cph$diarrhoea == 0 | is.na(cph$diarrhoea)) & 
                  (cph$vomiting == 0 | is.na(cph$vomiting) & 
                     (cph$bloody == 0 | is.na(cph$bloody))) ] <-  NA

# re-code sex in to binary
cph$sex <- ifelse(cph$sex == "male", 1, 0)
cph$sex <- as.numeric(cph$sex)

```

### Creating a case definition 

> Remember, in order to combine multiple filtering commands in to one selection you can use the "`|`" (bar not capital i) or "&" symbols. The **`|`** stands for **or** whereas the **`&`** stands for **and**. 


```{r}

# create new variable where people who have diarrhoea or vomiting get a 1 and all others a 0
cph$case <- 0
cph$case[cph$diarrhoea == 1 | cph$vomiting == 1] <- 1

# replace those who had onset before 11th nov 18:00 or after 13th nov 18:00

cph$case[cph$case == 1 & 
            ((cph$start == 1 & cph$starthour < 4) |
               (cph$start == 3 & cph$starthour > 3)| 
                 cph$start > 3)] <- 0

cph$case[is.na(cph$meal) | cph$meal == 0] <- NA

```


Do a plausibility check to see if everything worked

```{r}

# how many cases did you generate? 
table(cph$case)

```

\pagebreak

```{r}
# check if people were assigned properly according to symptoms

table(cph$case, cph$vomiting)
table(cph$case, cph$diarrhoea)

```


Drop cases that do not meet the case definition 

```{r}

cph <- cph[!is.na(cph$case), ]

```




### saving cleaned data 

You can save your cleaned dataset as an R datafile (.Rda) using the *save* command and re-load the same dataset using the *load* command. 
In reality you would never do this; your code should be stand-alone in that raw data is read and cleaned and analysis comes thereafter. 
But if you wanted to, this is how you would do it. 
```{r, eval = FALSE}

# save your dataset
save(cph, file = "cph.Rda")

# load your dataset 

load("cph.Rda")

```



\pagebreak 












## Descriptive analysis in *R*

If you haven't completed the previous section then you can load the following dataset for the next steps.


```{r}
# read in your data from a csv file 
# Select separator as comma (sep=",")
# do not import 'string' variables as 'Factors' (stringsAsFactors=FALSE) 
# Factors are a special datatype, covered later - character variables are simpler
# data frame read in and saved in R as "cph"


cph <- read.csv(here("Stata_data", "copenhagen_descriptive.csv"), sep=",", stringsAsFactors = FALSE) 
```




### Dataset description and tabulation   

### Describe signs and symptoms of cases  

There are two possible ways of doing this, one is to go through each of the symptoms using the table command (version 1). 
The other is to use a *for-loop*, which repeats the procedure for each of the variables defined (version 2). 

The *cbind* function attaches columns of datasets together. 



```{r}
  # Version 1

# create a frequency table diarrhoea among cases
a <- table(cph$diarrhoea[cph$case == 1], cph$case[cph$case == 1])

# frequencies divided by total cases (as proportion) and rounded to one decimal place

props <- round(a/215*100, digits = 1)

# bind a column (cbind) to the above table with:
  
b <- cbind(a, props)


# alternatively you can use the prop.table function instead of manual calcuation
  # b <- cbind(a, round(prop.table(a)*100, digits = 1))

```

\pagebreak

```{r}
  # Version 2

# define the variables which you would like to look at and save as "vars"
vars<-c("diarrhoea","bloody","vomiting","abdo","nausea","fever","headache","jointpain")

# start the for loop by saying, for all the variables saved in "vars" do:
for (var in vars){
  # create a table for one variable at a time among cases and save under "b". 
      # using squarebrackets: [select row, select column]
        # select rows which are cases (cph$case==1)
        # select columns of interest (vars)
  b <- table(cph[cph$case == 1,var], cph[cph$case == 1,"case"])
  # same as above to get proportions 
  b <- cbind(b,round(prop.table(b)*100, digits=1))
  
  # rename the columns of your output using colnames function 
  
  colnames(b) <- c("n", "%")
  
  # for interpretation purposes, print the name of the variable as a header in the output
  print(var)
  # print the table with frequency (column 1) and percentage (column)
  print(b)
  # repeats from the beginning until all the variables have been looped through
}

```


### Determine the median incubation period 

```{r}

summary(cph$incubation[cph$case == 1])

```

### Describe the cohort in terms of person 
```{r}
# look at age and other variables of interest
summary(cph$age)
table(cph$age)

```


### Calculate the overall attack rates as well as the attack rates stratified by person characteristic  

Remember, you can also refer to a dataset using square brackets, the part before a comma refers to the rows and after refers to columns (named or numerically). Or visually: cph[*row* **,** *column*].
For example: cph[ , "age"] gives you the variable age as a vector, or cph[2:4 , ] gives you rows two to four. 

In this case, the for-loop will replace *var* with the variables of interest we have defined in *vars*. 

```{r}

# students and sex
vars <- c("group","class", "sex")

# repeat for each of the variables in the list
for (var in vars){
  # create a basic two by two table
    # one variable at a time
  a <- table(cph[,var], cph$case)
  
  # create a table of proportions
  b <- round(prop.table(a, margin = 1) * 100, digits = 1)
  
  # bind the column with AR from b to the 2by2 table in a
  c <- cbind(a, b[,2])
  
  # rename your columns 
  
  colnames(c) <- c("non case", "case", "AR%")
  
  # print variable name to make output easier to understand
  print(var)
  print(c)
}

```


The same can be done with just a single variable 

```{r}
# total
# create a basic table
  a <- table(cph$case)
  
  # create a table of proportions
  b <- round(prop.table(a) * 100, digits = 1)
  
  # bind the column with AR from b to the 2by2 table in a
  total <- cbind(a, b)
  
  # rename colnames
  
  colnames(total) <- c("n", "AR%")

```



To see how to combine this code to create a ready-to-present table for exporting see the appendix. 


\pagebreak 
















# Describing time in *R*




### Recoding for date compatability 
Date compatibility is always an issue when switching between softwares, R sometimes has a problem reading in dates from excel files, however there is user-written code which fixes these kinds of glitches. 

In the current dataset it is simply a case of creating useful date variables by some data manipulation. 

The variable *start* is an example of bad data collection. 
Nevertheless it consists of numbers (1, 2 and 3); these numbers correspond to dates (11th, 12th and 13th of November 2006).



```{r}
# Create a useful date variable from start 


# set yourself a reference date (one day before onset)

refdate <- as.Date("2006-11-10") 

# use start to create dates from your start variable

cph$dayonset <- as.Date(refdate + cph$start)


```


### Construct epicurve by day
It is possible to create an epicurve manually using base-R code (see appendix), however for simplicity, there is a user written function (Daniel Gardener, PHE). 
The epicurve function allows creation of easily formatted epicurves. To find out more about the function, first load it as above and then click on function in the Global Environment tab on the right of the R Studio window.
This function uses the gpplot package (use ??ggplot2 to find out more about this function) and so is quite easily manipulated and saved after being created.



```{r, warning = FALSE}
epicurveday <- epicurve(cph, date.col = "dayonset", time.period = "day",
                start.at = "2006-11-10", stop.at = "2006-11-15",
                xlab = "Date of symptom onset", ylab = "Count",
                col.pal = 4, label.breaks = 0, epi.squares = TRUE, na.rm = TRUE)



# As epicurveday is a ggplot object, it is possible to tailor it as desired
epicurveday <- epicurveday +
                  # rotating the x axis label by 90               
                  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
                  # adding a title
                  ggtitle("Gastroenteritis cases by date of onset, November 2006") +
                   # centring the title and reducing its size 
                  theme(plot.title = element_text(hjust = 0.5, size = 11)) 

epicurveday


```


### Recoding for time compatibility (6 hour intervals)
Clearly the 24 resolution does not reveal anything valuable. Try again using a 6 hour time scale. 

For this we are going to combine variables to create a character variable using the *paste0* function. 
The *paste0* funciton is quite handy because it allows you to insert different bits of text or numbers in to one string. 

Now we could make this in to a date-time variable; but for the purpose of plotting, it is easier to just leave as a character. 


```{r, tidy= T}

# combine your new start date with a starting hour
cph$timeonset <- paste0((refdate + cph$start), " ", (cph$starthour - 1) * 6)

# those with missing onset should also be missing here
cph$timeonset[is.na(cph$start)] <- NA


# Select the order of the timegroupings you want to show by making it a factor
  # (For a more complicated way of doing this, see appendix)

cph$timeonset <- factor(cph$timeonset, levels = c("2006-11-10 0", "2006-11-10 6", "2006-11-10 12", "2006-11-10 18", 
                                                  "2006-11-11 0", "2006-11-11 6", "2006-11-11 12", "2006-11-11 18",
                                                  "2006-11-12 0", "2006-11-12 6", "2006-11-12 12", "2006-11-12 18", 
                                                  "2006-11-13 0", "2006-11-13 6", "2006-11-13 12", "2006-11-13 18", 
                                                  "2006-11-14 0", "2006-11-14 6", "2006-11-14 12", "2006-11-14 18"))


```





\pagebreak


### Construct epicurve by hour grouping
  
Using the epicurve function it is possible to choose what breakdown you would like on your time-axis by setting time.period to be "use.date.col.as.is". 


```{r, fig.width=20, fig.height=10, tidy=T}

hourlycurve <- epicurve(cph[!is.na(cph$timeonset),], date.col = "timeonset", time.period = "use.date.col.as.is",
                       xlab = "Date and time of symptom onset", ylab = "Count",
                       epi.squares = TRUE, na.rm = TRUE)


hourlycurve

```


\pagebreak




















# Univariable analysis (Cohort and case control)

To identify (the) potential vehicle(s) in the outbreak, proceed with an analytical study where you use statistical tests to investigate the associations of some suspicious food items with the disease.


If you haven't completed the previous section then you can load the following dataset for the next steps.


```{r}
# read in your data from a csv file 
# Select separator as comma (sep=",")
# do not import 'string' variables as 'Factors' (stringsAsFactors=FALSE) 
# Factors are a special datatype, covered later - character variables are simpler
# data frame read in and saved in R as "cph"


cph <- read.csv(here("Stata_data", "copenhagen_univariable.csv"), sep=",", stringsAsFactors = FALSE) 
```



There is more than one way to create univariable tables.  One gives you a two by two and quite a lot of info, and the other is user-written code which will provide you with nice output like STATA's cstable. 



Think about which variables you might want to include when checking for effect modification or confounding. One common strategy is to base this decision on the univariable results obtained and a p-value threshold of 0.25. Also, food items that are known risk factors for gastroenteritis could also be included regardless of their univariable p-value.


## Detailed output (method 1)

In order to use the epi.2by2 function, we first need to convert the outcome and exposure variables into factor variables to facilitate interpretation.  

Outcome and exposure variables of interest need to be factor variables prior to using the function, in order to be relevelled from (0,1) to (1,0) so that they can be correctly organised in 2-by-2 tables.

```{r}
# We list the outcome/exposure variables
vars <- c("shrimps", "veal", "pasta", "sauce", "champagne", "rocket", "case")


# Convert all of those variables to factor variables and re-order the levels to aid interpretation
for (var in vars) {
  cph[,var] <- factor(cph[,var],levels = c(1,0)) 
}
```


The *epi.2by2* function from the *epiR* package can be used to calculate both RRs and ORs.
You can find out more information on the function by writing **?epi.2by2** in the console. 
The epi.2by2 function requires data to be in a table format. 
We can specify that we want to calculate RRs or ORs by adding **method = "cohort.count"** or **method = "case.control"**, respectively. 
You can do that first for looking at the risk for being a case by exposure to shrimp in a cohort study design.  

\pagebreak


```{r}
# Create a table with exposure and outcome variables
counts <- table(cph$shrimps, cph$case)

# Apply epi.2by2 function to the table
shrimp <- epi.2by2(counts, method = "cohort.count")

# to view the output
shrimp

```


This example involves using the *epi.2by2* function from the *epiR* package. To make things faster (but not necessarily easier to understand) it has been run in a for-loop and the output has been hidden because it is very long. 

With extra code you could extract information from epi.2by and piece together an output table like in STATA.
Here we have just chosen to show this in the console (extraction will be shown later in the case study).

```{r, results="hide"}

## vector of variables you want to run in the for-loop
vars <- c("pasta","veal", "champagne", "sauce", "shrimps")

# for each variable run a table and then get the rr out using epi.2by2 from the epiR package

for (var in vars){
  # create table with the variable of interest by case classification
  a <- table(cph[,var], cph$case)
  
  # pass your counts table to epi.2by2
  test <- epi.2by2(a,  method = "cohort.count")
  
  # print the output of the table in the console and label which variable it is
  print(var)
  print(test)
}

```




\pagebreak


## Simplified output (method 2)

A much more straight forward way is to simply apply a user-written function which requires much less input from you. 
The *sva* function (Daniel Gardner, PHE) basically integrates the steps from above to create a nice output table. 

```{r}
# You already sourced the function at the begining of this case study 
# if you havent then type: source(here("scripts", "single.variable.analysis.v0.2.R"))
# nb. click on "sva" in your global environment to view source code and read explanations

# cohort study
sva(cph, outcome="case", exposures=c(vars), measure="rr", verbose=TRUE)

# cohort count
sva(cph, outcome="case", exposures=c(vars), measure="or", verbose=TRUE)

```





# Different statistical tests in *R*


Note that for the below tests, no tables are printed alongside, however you could create these tables using the table and propstable (for percentages) functions. 


## Is gender associated with being a case? 

```{r}

# using a chi-squared test
  # chisq.test funciton requires you to input a table
chisq.test(table(cph$sex, cph$case))


# using a fisher's exact test

fisher.test(table(cph$sex, cph$case))


```



## Is class associated with being a case?
Here you can either compare proportions, using the chi-squared test, or use the Wilcoxon-Mann-Whitney test to compare distributions. 
For the Wilcoxon-Mann-whitney test to work, all variables need to be numeric - however it is not possible to go directly from a factor to a numeric. The intermediate is to turn it in to a character and then to a numeric. 
```{r}

# Using chi-squared
chisq.test(table(cph$class, cph$case))

# using the wilcoxon test 
  # this is a ranksum test and the function pulls the counts numbers by itself

# change case from a factor to numeric
cph$case <- as.numeric(as.character(cph$case))

# run the wilcoxon 
wilcox.test(cph$class, cph$case)

```

## Is there a difference between age in males and females? 
```{r}


# Using a wilcox test
wilcox.test(cph$age, cph$sex)

# Using a t-test to see difference in mean if normally distributed 
  # shapiro tests if significantly different from normal distribution
shapiro.test(cph$age)

t.test(cph$age ~ cph$sex)


```




## Is there a dose-response relationship between the food items and being sick? 

Look at the food items that seem most suspicious to you. 
Hint: it's pasta and veal

There is two possible ways to do this. The first is to use the wilcox.test again, and the second is to calculate ORs using the epi.2by2 function. Using the epi.2by2 function for dose responses is a bit more dense (traditionally this would be done using regression), so we have not shown it here; if you would like to see the code for this it is available in the appendix. 



```{r}


vars <- c("vealD", "pastaD")


for (var in vars){

  # selects only those cases where dose response is filled out (not empty) and saves as a dataset
  dataset <- cph[is.na(cph[var]) == FALSE,]
  
  # saves in "form" as a text 
      # e.g. "pastaD ~ case" which can then be inserted in the wilcox.test to get dose response
  form <- as.formula(paste0(var,"~","case"))
  
    # put your formula above in to wilcox using the reduced dataset
  a <- wilcox.test(form, data = dataset)
  
    # show a table with doses and case and the wilcox output
  print(var)
  print(table(cph[,var], cph$case))
  print(a)
}

```










# Stratified analysis 


You can see that those eating pasta or veal as well as drinking champagne have the highest risk of becoming ill. There are, however, many other food items that are associated with an increased risk (even if not statistically significant). At this stage we cannot conclude anything, but need to check for effect modification and confounding. This should be done by stratification.



If you haven't completed the previous section then you can load the following dataset for the next steps.


```{r}
# read in your data from a csv file 
# Select separator as comma (sep=",")
# do not import 'string' variables as 'Factors' (stringsAsFactors=FALSE) 
# Factors are a special datatype, covered later - character variables are simpler
# data frame read in and saved in R as "cph"


cph <- read.csv(here("Stata_data", "copenhagen_stratified.csv"), sep=",", stringsAsFactors = FALSE) 
```




Make sure that your variables of interest are factors in the correct order. 
Outcome and exposure variables of interest need to be factor variables prior to using the function, in order to be relevelled from (0,1) to (1,0) so that they can be correctly organised in 2-by-2 tables. (We have already done this for our variables)

```{r}
# We list the outcome/exposure variables
vars <- c("shrimps", "veal", "pasta", "sauce", "champagne", "rocket", "case")


# Convert all of those variables to factor variables and re-order the levels to aid interpretation
for (var in vars) {
  cph[,var] <- factor(cph[,var],levels = c(1,0)) 
}
```




Stata users could use the csinter function to identify effect modifiers/confounders. 
The epi.2by2 function in the epiR package provides similar functionality. 


```{r}


# Make a 3-way table with exposure of interest, the outcome and the stratifying variable, in that order
a <- table(cph$veal, cph$case, cph$pasta)

# Use the epi.2by2 function to calculate RRs (by stating method = "cohort.count")
mhtable <- epi.2by2(a, method = "cohort.count")

# view the output
mhtable

```


You can then extract various outputs from the epi2by2 table for piecing together your own output. 
Here two levels of $-signs are needed, as the function saves outputs to a list. 

```{r}

# Crude RR
mhtable$massoc$RR.crude.wald 

# Stratum specific RRs
mhtable$massoc$RR.strata.wald

# Adjusted RR
mhtable$massoc$RR.mh.wald

# You can combine all of those elements in to a single table using rbind
results <- rbind(mhtable$massoc$RR.crude.wald, 
                          mhtable$massoc$RR.strata.wald, 
                          mhtable$massoc$RR.mh.wald)


# We can label the rows of this table as below
rownames(results) <- c("Crude", "Strata 1", "Strata 0", "Adjusted")

# view output table
results

```














You can then piece these parts together within a for loop to investigate more variables 



```{r}


# We list the exposure variables
vars <- c("veal", "rocket", "shrimps", "champagne", "sauce")



# Create an empty list to save the output of the loop
  # A list is a collection of dataframes
outputs <- list()

# for each of the exposures of interest, run the stratified analysis from above
for (var in vars) {
  b <- table(cph[,var], cph$case, cph$pasta)
  mh <- epi.2by2(b, method = "cohort.count")
  resultstable <- rbind(mh$massoc$RR.crude.wald, 
                          mh$massoc$RR.strata.wald, 
                          mh$massoc$RR.mh.wald)
  rownames(resultstable) <- c("Crude", "Strata 1", "Strata 0", "Adjusted")
  
  # save your results table as a dataframe within your outputs list 
  outputs[[var]] <- resultstable
}

# view your results
outputs # Gives crude, stratum-specific and adjusted RRs

```


\pagebreak

The exact same can be done for veal (switch veal and pasta)


```{r, eval = FALSE}


# We list the exposure variables
vars <- c("pasta", "rocket", "shrimps", "champagne", "sauce")



# Create an empty list to save the output of the loop
outputs2 <- list()

# for each of the exposures of interest, run the stratified analysis from above
for (var in vars) {
  b <- table(cph[,var], cph$case, cph$veal)
  mh <- epi.2by2(b, method = "cohort.count")
  resultstable <- rbind(mh$massoc$RR.crude.wald, 
                          mh$massoc$RR.strata.wald, 
                          mh$massoc$RR.mh.wald)
  rownames(resultstable) <- c("Crude", "Strata 1", "Strata 0", "Adjusted")
  
  # save your output table as a dataframe within your list (of dataframes)
  outputs2[[var]] <- resultstable
}

# view your results
outputs2 # Gives crude, stratum-specific and adjusted RRs

```




It appears that pasta confounds the association between eating veal and being 
a case. For a variable to be a confounder it needs to be associated both with 
the outcome (being a case) and with the exposure. We know from univariable 
analysis that pasta is associated with being a case, we can now check if it is 
also associated with veal.

```{r}
# using a fisher's exact test

fisher.test(table(cph$pasta, cph$veal))
```




# Integrate the analysis steps in one master script file
This is easily done, if you have saved each of your analyses in a seperate script (e.g. one for cleaning, descriptive, univariable and stratified each), you can create a master script by simply using the *source* function (as we did to read in user-written code) to run each of your scripts


\pagebreak



















# Appendix 


## Reading in datafiles to *R* from other formats (STATA and Excel)

In *R* you can read datasets saved in STATA format (.dta), using a package called "haven". 
See *??read_dta* for details

```{r, eval = FALSE}

library(haven)

cph <- read_dta("Copenhagen/Session 3/Copenhagen3.dta")

```



It is also possible to read in excel files using a package called "readxl". 

```{r, eval = FALSE}
library("readxl")
cph <- read_excel("Copenhagen/Session 3/Copenhagen3.xlsx")
```





## Optional task - Export your output to excel 
This section produces a full descriptive table and exports it to excel.
This is just to illustrate that you can create tables however you would like to. 
If you are feeling confident then take a look at the code - if not just skip it!

<!-- ZNK: it may be better to write to csv as opposed to excel? -->

```{r, eval = FALSE}

a<-matrix(NA, ncol = 4, nrow = 8)

rownames(a) <- c("student","teacher", "1", "2", "3", "male", "female", "total")

colnames(a) <- c("Number", "% of Total", "Number of cases", "Attack rate (in %)")

for (var in vars){
  b<-table(cph[,var], cph$case)
  
  catTotal <- rowSums(b)
  
  denom <- sum(b)
  
  props <- round(catTotal/denom * 100, digits = 1)
  
  cases <- b[,2]
  
  AR <- round(cases/catTotal *100, digits = 1)
  
  a[rownames(a) %in% names(catTotal), "Number"]  <- catTotal
  a[rownames(a) %in% names(props), "% of Total"]  <- props
  a[rownames(a) %in% names(cases), "Number of cases"]  <- cases
  a[rownames(a) %in% names(AR), "Attack rate (in %)"]  <- AR
}



a

z<-data.frame(unclass(a))

write.xlsx(z, "C:/Users/Desktop/Outbreak module 2015/Copenhagen/table.xlsx", sheetName="table")


```






## Manual epicurve construction using base plotting functions

Creating an epicurve using the builtin *barplot* function is one option, as it is relatively easy to define the time axis - though getting boxes per case is a bit of a hassle.  
Note: several lines of code have been turned to comments (# png(...) and # dev.off()), these pieces of code are used to save the plot as a picture file on your desktop, however you can run the code in your R browser without saving the picture output to a folder. 



```{r, tidy=T, eval = FALSE}
# table to use in plotting 
count<-table(cph$startdate)

# define parameters to use in the image file (.png) after plotting (see ?png for help)
aa <- 2
h1=1800*aa
w1=sqrt(2)*h1
r1=300 # 300
ps1 <- 20


## define where and what you want to save your output picture file as
# png(filename="C:/Users/Desktop/Outbreak module 2015/Copenhagen/epicurve.png",width=w1,height=h1,res=r1,pointsize=ps1)

# regular bar plot 
epicurve<-barplot(count, beside = TRUE, axes = FALSE, xaxt = "n", xlab = NULL, legend.text = NULL, col = "red", space = c(0,0))

# axis numbering 
axis(2, at = seq(0, max(count), by = 10),tick = TRUE, pos = 0, las = 1, cex = 1.5)

axis(1, at = c(epicurve), labels = rownames(count), tick = TRUE, pos = 0)

# another way of doing axis labels 
mtext(text="Cases (n)",
      side=2,
      line=3,
      cex=1.5)

# turns off the plotting function and saves the output picture
# dev.off()


```





## Creating time levels with coding rather than typing it out 

```{r, eval = FALSE}

a <- expand.grid(c("0","6","12", "18"), paste0("2006-11-", 10:14))

b <- paste0(a$Var2," ", a$Var1)


cph$starttime <- factor(cph$starttime, levels = b)
```



## Using epi.2by2 for investigating dose-response relationships


Using epi.2by2 to get odds ratios for a dose response relationship is a bit more dense than what we did above, as you need to patch together tables to feed in to the function for each dose-level. 
Here you need case to be a factor again. 
(it would of course be much easier to use regression for this, but this demonstrates some datahandling techniques)



```{r}

# change case back to a factor 
cph$case <- factor(cph$case,levels = c(1,0)) 


# create your table of counts 

counts <- table(cph$pastaD, cph$case) 

# create an empty matrix to fill with your output
  # this matrix should be the same number of rows as your counts dataset
  # you also want 3 columns to put your output in to
output <- matrix(NA, nrow = nrow(counts), ncol = 3)

# for each of the dose response levels
for (i in 2:nrow(counts)) {
  
  # create your 2by2 counts table by combining the ref group with one dose level (row)
  input <- rbind(counts[1,], counts[i,])
  
  # feed this table to epi2by2
  test <- epi.2by2(input,  method="case.control")
  
  # overright the row of your output matrix with the relevant estimates
  output[i,] <- as.numeric(test$massoc$OR.strata.wald)

}

# name the rows of your output accordingly
colnames(output) <- c("OR", "lower", "upper") 


# bind your output table to your counts table

counts <- cbind(counts, output)

```



\pagebreak 

# Conclusions of the case study 


### Descriptive analyses 

We didn't find anything surprising in the descriptive analysis of the cohort's age given it consists of two groups, the students and the teachers.

The distribution of the cohort regarding sex, group and class also didn't reveal anything unusual. Students seem a bit more affected by the outbreak than teachers and the attack rate is higher for older students in higher classes. This, however, is a purely descriptive result.

When constructing an epicurve, we need to decide on the resolution, i.e. the time interval for a single bar. A rule of thumb is to use one third or one fourth of the average incubation period as an interval. For our investigation this means we should use approximately a 6h interval.

This seems a good choice indeed as we saw that the daily interval was too coarse to really see the signal we're after. The epicurve and the summary of the incubation period show that there seemed to be a rapid onset of symptoms following exposure. This is in line with our previous suspicion that a virus or a toxin might be the causative agent in the outbreak.

The unimodal shape with the sharp peak suggests a point source, while the tail on the right hand side could be explained by secondary cases or background noise. Also people that only consumed a little contaminated food and therefore only a low infectious dose could have a longer incubation period and could explain the late cases.
The above results are in line with norovirus as the prime suspect, but the symptoms are not a textbook fit. There are too few people that experienced vomiting!


## Univariate analyses 
The interesting results here are that the two food items that are most suspicious are pasta and veal. In particular, the dose response relationship that was found for pasta points towards pasta as the potential vehicle. Pasta as such is unlikely to be contaminated, but as you can see in the label, it was served with pesto!
Before you jump to conclusions, be aware that this result could be due to confounding! Maybe pasta was clean but eaten by all the people who ate the food item that actually was contaminated!

## Stratified analyses 
We found that pasta consumption confounds the association between eating veal and being ill. The crude (univariable) result for veal suggests that veal is a risk factor (crude RR = 1.52, CI: 1.00-2.31), but when we adjust for the consumption of pasta we see that actually this result is due to the fact that most people who ate veal also ate pasta.
Within the stratum of the people who ate pasta, veal has no effect (RR = 1.19, CI: 0.59-2.40). The same holds within the stratum of people who didn't eat pasta (RR = 1.05, CI = 0.38, 2.92). This is why the adjusted MH-RR also suggests that veal has no effect (RRadj = 1.14, CI: 0.64-2.03).
This result taken together with the dose response relationship we found earlier for pasta gives additional evidence that there was something going on with the pesto!



## Microbiological analyses 

### Local clinical microbiology laboratory

The finding of Salmonella spp. is surprising because one would expect a median incubation period of at least 24 hours in a Salmonella outbreak. A look at the epidemic curve tells us that the median incubation period in this outbreak was <18 hours. It would also be unusual to have such a low proportion of positive stool samples (3 of 20), if Salmonella was the only pathogen causing the outbreak.
Most microbiology laboratories use conventional methods (culture and microscopy) for routine detection of common bacterial enteric pathogens; molecular approaches are used by specialised laboratories to screen for key pathogens and their virulence genes.
Further microbiological investigation is required involving specialised assays that are only usually undertaken by a reference laboratory, namely, detection assays for diarrhoeagenic Escherichia coli and norovirus. All stool samples implicated in the outbreak should be examined for these pathogens also.

### Reference laboratory results 

The fact that this was an outbreak where several aetiological agents were identified, may point to a contamination from an environmental source. It also makes it less likely that kitchen staff excreting bacteria could have contaminated the food (and we already know that kitchen staff claimed not to have had symptoms, and that their stools tested negative).
ETEC is among the leading bacterial causes of diarrhoea in the developing world in particular among children, as well as the most common cause of travellers' diarrhoea. It is increasingly being recognised as an important cause of diarrhoea also in the developed world. ETEC is transmitted by food or water contaminated with human (or maybe animal) faeces.
ETEC is defined by the expression of one or more enterotoxins. Two such classes of toxins exist, heat-labile enterotoxin (LT) and heat-stable enterotoxin (ST). ETEC may produce either LT, ST or both. Both toxins directly cause diarrhoea, but other virulence factors also exist. LT is similar to the cholera toxin.
Infection with ETEC can cause profuse watery diarrhea and abdominal cramping. Illness develops somewhat quicker than for other bacterial enteric infections, because of the presence of the toxins. Symptoms appear 1-3 days after exposure and usually last 3-4 days. Some infections may take a week or longer to resolve. Symptoms rarely last more than 3 weeks. Most patients recover with supportive measures alone and do not require hospitalization or antibiotics.
Diagnostics is not easily performed; most laboratories do not test for ETEC.

Note that the field of diarrhoeagenic E. coli involves other types also. E. coli constitute a large group of bacteria that occur naturally in the intestines of humans and animals. Most are non-pathogenic. However, several (six are generally recognized) groups of diarrhoea causing E. coli, exist. These include ETEC, but also other groups, among which are:


+EPEC (enteropathogenic E. coli), cause diarrhoea through an infectious mechanism not involving toxins. It mostly affects children under the age of 2. Is transmitted from person-to-person, sometimes via foods.
+ STEC, (shiga toxin producing E. coli), STEC (also known as VTEC and EHEC) are similar to EPECs in many ways, but additionally express shiga toxins. Because of this, these strains may cause HUS, haemolytic uremic syndrome.

### Food sample results

ETEC can be very difficult to isolate from food samples. Not isolating ETEC therefore does not exclude that ETEC was present in the sample. It is just extremely difficult to find it among all the other E.coli (a bit like looking for a needle in a haystack). In fact the presence of very high counts of generic E. coli in the food, suggests that ETEC was also there.
One strategy to increase the success would be to use an enrichment step. That means letting the bacteria grow - before trying to isolate ETEC - for some hours under conditions, where growth of ETEC would be favoured above other types of E. coli (if such conditions can be established). Or, if for instance it was known that the ETEC strain isolated from cases was resistant to a specific set of antibiotics, this could be utilized in the isolation process, by growing the food extracts in a medium containing these antibiotics. An alternative strategy would be to use PCRs directly addressing the ETECs or to detect the toxins directly. However, as the pathogenic ETEC likely were present in small amounts, this would also be difficult.

### PFGE results 

The four Salmonella Anatum strains were found to have the same PFGE profile and 100% identity on the dendogram. The S. Anatum isolate found in the food was also typed by PFGE and found to have the same profile. The other strains show greater diversity within the dendogram confirming that they are distinguishable strains.
Concerning ETEC, sixteen of the 17 O92:H- isolates were indistinguishable by PFGE whereas the PFGE profile of the remaining O92:H- isolate differed from the others by a few bands.

## Conclusions of the investigation 
In summary, it is fair to say that there was both epidemiological and microbiological evidence that the pasta salad with pesto was the most likely vehicle of transmission in this outbreak. Further investigations focused on how the pasta salad with pesto could have become contaminated and on lessons learned from this outbreak that could then communicated both the scientific community, the caterer and the general public.


